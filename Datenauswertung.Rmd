---
title: "R Notebook"
output: html_notebook
---

# .............................................................................. ----
# style [Style Regeln, damit unser Code gleich aussieht und von allen gelesen werden kann]

```{r}

# First of all some style rules. Headers get # and a line # ...... ---- above
# it, Subheaders get ##, subsubheaders get ###. All headers get ---- at the end
# so that they are foldable. Collapse all with Alt+o to see its beauty
# (Alt+Shift+o to expand all afterwards) Regular comments that are not headers
# get a single #

## about spacing

#bad
# good


## Two blank lines before each header you can fold and one afterwards ----

# one blank line after each header. No blank lines after a normal comment
bad<-"bad"
good <- "good"

# one blank before each comment.

# Vor und nach ``` Leerzeichen lassen

```

# .............................................................................. ----
# load packages & checkpoint

```{r}

if(!require("checkpoint")){install.packages("checkpoint")}; library("checkpoint") # this solves the problem of package reproducibility. It saves current versions of the scripts you use.

# checkpoint("2020-11-03", checkpointLocation = getwd())

if(!require("tidyverse")){install.packages("tidyverse")}; library("tidyverse")
if(!require("ggplot2")){install.packages("ggplot2")}; library("ggplot2") # plots
if(!require("lubridate")){install.packages("lubridate")}; library("lubridate") # Umgang mit Datum
if(!require("readr")){install.packages("readr")}; library("readr") # Einlesen von Daten
if(!require("dplyr")){install.packages("dplyr")}; library("dplyr") # Einlesen von 
if(!require("e1071")){install.packages("e1071")}; library("e1071") # Support Vektor Machines
if(!require("Metrics")){install.packages("Metrics")}; library("Metrics") # Support Vektor Machines

```

# .............................................................................. ----
# load data [hier gehts los]

```{r}

umsatzdaten <-read_csv("raw_data/umsatzdaten_gekuerzt.csv")
kiwo <- read_csv("raw_data/kiwo.csv")
wetter <- read_csv("raw_data/wetter.csv")
uebernachtungen <- read.csv("raw_data/Uebernachtungen_Kiel_Mai-August_2019.csv",sep = ";")
holstenstrasse <- read.csv("raw_data/kiel-holstenstrasse_2020.csv", sep = ";")

print(umsatzdaten)
```

# .............................................................................. ----
# Datensätze strukturieren und auf Fehler untersuchen [hier gehts los]

weitere Fragem an den Datensatz:
Sind alle Tage enthalten oder gibt es auch Tage die nicht enthalten sind? Gibt es Wochen an denen frei/kein umsatz war/ist?


```{r}

## Überprüfen auf NA 
which(is.na(umsatzdaten==TRUE))
which(is.na(kiwo==TRUE))
which(is.na(wetter==TRUE))

# Es gibt NAs im wetter Datensatz, in den beiden anderen nicht.

```

```{r}

## Erweitern des Umsatzdaten Datensates mit Wochentagen
umsatzdaten$weekday <- wday(umsatzdaten$Datum) # 1 ist Sonntag. Meine ich.

## Wetter an den Datensatz anschließen
umsatzdaten <- merge(umsatzdaten, wetter, by = "Datum")

## Umsatz des Vortages zufügen -> Vorsicht mit Warengruppe!
umsatzdaten %>% group_by(weekday) # klappt so nicht. Muss mit Group_by gemacht werde

```

# .............................................................................. ----
# Lineare Regression 
```{r}

```


```{r}

q <- lm( formula = Umsatz ~ Temperatur + Bewoelkung + weekday ,data = umsatzdaten)
summary(q)

```


# .............................................................................. ----
# Support Vektor Machine Analysis

## TODO
Hier sind noch die Fragen offen: Welche Variablen sollten wir setzen?


## Aufteilung des Datensatzes in Trainings- und Testdaten

```{r}
# Zufallszähler setzen (um die zufällige Partitionierung bei jedem Durchlauf gleich zu halten)
set.seed(1)

# Zufällige Ziehung von Indizes für die Zeilen des Datensatzes, die dem Traininsdatensatz zugeordnet werden
indices_train <- sample(seq_len(nrow(umsatzdaten)), size = floor(0.80 * nrow(umsatzdaten)))

# Definition des Trainings- und Testdatensatz durch Selektion bzw. Deselektion der entsprechenden Datenzeilen
train_dataset <- train_dataset_org <- umsatzdaten[indices_train, ]
test_dataset <- umsatzdaten[-indices_train, ]
```


## Data Preparation

```{r}
train_dataset <- sample_frac(train_dataset_org, .10)
```


## Training the SVM

```{r}
# Estimation of an SVM with optimized weighting parameters and given standard hyper parameters
# Typically not used; instead, the function svm_tune is used in order to also get a model with optimized hyper parameters
model_svm <- svm(Umsatz ~ Bewoelkung, train_dataset)
```

```{r}
# Estimation of various SVM (each with optimized weighting parameters) using systematically varied hyper parameters (typically called 'grid search' approach) and cross validation
# the resulting object includes the optimal model in the element named 'best.model'
svm_tune <- tune(svm, Umsatz ~ Bewoelkung + Temperatur + Windgeschwindigkeit + weekday, data=train_dataset,
                 ranges = list(epsilon = seq(0.2,1,0.1), cost = 2^(2:3))) #was machen mit ranges? Andere Werte wählen?
```


## Checking the Prediction Quality


### Trainig Data

SVM without cross validation and grid Search
```{r}
# Calculating the prediction for the training data using the best model according to the grid search
pred_train <- predict(model_svm, train_dataset)
# Calculating the prediction quality for the training data using the MAPE
mape(train_dataset$Umsatz, pred_train)

```

SVM with cross validation and grid Search
```{r}
# Calculating the prediction for the training data using the best model according to the grid search
pred_train <- predict(svm_tune$best.model, train_dataset)
# Calculating the prediction quality for the training data using the MAPE
mape(train_dataset$Umsatz, pred_train)
```

### Test Data

SVM without cross validation and grid Search
```{r}
# Calculating the prediction for the test data using the best model according to the grid search
pred_test <- predict(model_svm, test_dataset)
# Calculating the prediction quality for the test data using the MAPE
mape(test_dataset$Umsatz, pred_test)
```

SVM with cross validation and grid Search
```{r}
# Calculating the prediction for the test data using the best model according to the grid search
pred_test <- predict(svm_tune$best.model, test_dataset)
# Calculating the prediction quality for the test data using the MAPE
mape(test_dataset$Umsatz, pred_test)
```
```{r}
# predict Umsatz for the specific date 05-06-2019
unknowndate <- data.frame(umsatzdaten_Datum = c(05-06-2019))

predict(svm_tune$best.model,newdata=unknowndate)
```

## Neuronales Netz: 

```{r}
install.packages("reticulate")
library(reticulate)

# Installation von miniconda (falls nicht vorhanden)
install_miniconda(update=TRUE)

# Anlegen einer speziellen Python Umgebung
conda_create("r-reticulate")

# Installieren der Pakete in der angelegten Umgebung
conda_install("r-reticulate", "pandas")
conda_install("r-reticulate", "numpy")
conda_install("r-reticulate", "tensorflow")
conda_install("r-reticulate", "h5py")
 
# Verwenden der speziellen Python Umgebung die zuvor erstellt wurde
use_condaenv("r-reticulate")

```
Einfügen des Skripts von steffen Source() hat nicht funktioniert

```{r}
###################################################
### Preparation of the Environment ####

# Clear environment
remove(list = ls())


# Create list with needed libraries
pkgs <- c("readr", "fastDummies")

# Load each listed library and check if it is installed and install if necessary
for (pkg in pkgs) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg)
    library(pkg, character.only = TRUE)
  }
}


###################################################
### Function Definition ####

#' Title Fast creation of normalized variables
#' Quickly create normalized columns from numeric type columns in the input data. This function is useful for statistical analysis when you want normalized columns rather than the actual columns.
#'
#' @param .data An object with the data set you want to make normalized columns from.
#' @param norm_values Dataframe of column names, means, and standard deviations that is used to create corresponding normalized variables from.
#'
#' @return A data.frame (or tibble or data.table, depending on input data type) with same number of rows an dcolumns as the inputted data, only with normalized columns for the variables indicated in the norm_values argument.
#' @export
#'
#' @examples
norm_cols <- function (.data, norm_values = NULL) {
  for (i in 1:nrow(norm_values)  ) {
    .data[[norm_values$name[i]]] <- (.data[[norm_values$name[i]]] - norm_values$mean[i]) / norm_values$sd[i]
  }
  return (.data)
}


#' Title Creation of a Dataframe including the Information to Standardize Variables
#' This function is meant to be used in combination with the function norm_cols
#'
#' @param .data A data set including the variables you want to get the means and standard deviations from.
#' @param select_columns A vector with a list of variable names for which you want to get the means and standard deviations from.
#'
#' @return A data.frame (or tibble or data.table, depending on input data type) including the names, means, and standard deviations of the variables included in the select_columns argument.
#' @export
#'
#' @examples
get.norm_values <- function (.data, select_columns = NULL) {
  result <- NULL
  for (col_name in select_columns) {
    mean <- mean(.data[[col_name]], na.rm = TRUE)
    sd <- sd(.data[[col_name]], na.rm = TRUE)
    result <- rbind (result, c(mean, sd))
  }
  result <- as.data.frame(result, stringsAsFactors = FALSE)
  result <- data.frame (select_columns, result, stringsAsFactors = FALSE)
  names(result) <- c("name", "mean", "sd")
  return (result)
}



###################################################
### Data Import ####

# Reading the data file
umsatzdaten <- read_csv("data/umsatzdaten.csv")
umsatzdaten$X1 <- NULL # Liest eine Spalte zuviel ein, die löschen wir hier.


###################################################
### Data Preparation ####

# Recoding of the variables into one-hot encoded (dummy) variables

dummy_list <- c("Warengruppe", "weekday")
umsatzdaten_dummy = dummy_cols(umsatzdaten, dummy_list)

# Definition of lists for each one-hot encoded variable (just to make the handling easier) Alles was nicht sinnvoll normalisiert werden kann, wird hier eingespeist
weekday_dummies = c("weekday_1", "weekday_2", "weekday_3", "weekday_4", "weekday_5", "weekday_6", "weekday_7")
Warengruppe_dummies = c('Warengruppe_1', 'Warengruppe_2', 'Warengruppe_3', 'Warengruppe_4','Warengruppe_5', "Warengruppe_6")


# Standardization of all variables (features and label)

norm_list <- c("Umsatz", "Bewoelkung", "Temperatur", "Windgeschwindigkeit", "Wettercode", "umsatz_day_before", weekday_dummies, Warengruppe_dummies) # list of all relevant variables

norm_values_list <- get.norm_values(.data= umsatzdaten_dummy, norm_list)    # Calculation of the means and standard deviations

umsatzdaten_norm <- norm_cols(umsatzdaten_dummy, norm_values_list) # Standardization of the variables


###################################################
### Selection of the Feature Variables and the Label Variable ####

# Selection of the features (the independent variables used to predict the dependent)
features <- c("Bewoelkung", "Temperatur", "Windgeschwindigkeit", "Wettercode", "umsatz_day_before", weekday_dummies, Warengruppe_dummies)
# Selection of the label (the dependent variable)
label <- 'Umsatz'


###################################################
### Selection of Training and Validation data ####

# Setting the random counter to a fixed value, so the random initialization stays the same (the random split is always the same)
set.seed(1)
# Generating the random indices for the training data set
train_ind <- sample(seq_len(nrow(umsatzdaten_norm)), size = floor(0.66 * nrow(umsatzdaten_norm)))

# Splitting the data into training and validation data and selecting the feature variables as a separate data frame
train_dataset = umsatzdaten_norm[train_ind, features]
test_dataset = umsatzdaten_norm[-train_ind, features]

# Splitting the data into training and validation data and selecting the label variable as a separate vector
train_labels = umsatzdaten_norm[train_ind, label]
test_labels = umsatzdaten_norm[-train_ind, label]


```



### Benötigte packages laden:
```{r}
library(reticulate)
library(ggplot2)
library(Metrics)
```

### Datenquelle für die Vorhersage mit dem Neuronalen Netz:



### Definition des neuronalen Netzes

```{python}
# Benoetigte Python Libraries einbinden
import numpy as np
import tensorflow as tf
from tensorflow import keras

# Definition der Form des tiefen neuronalen Netzes (Deep Neural Net)
model = tf.keras.Sequential([
  keras.layers.Dense(10, activation='relu', input_shape=[len(r.train_dataset.keys())]),
  keras.layers.Dense(4, activation='relu'),
  keras.layers.Dense(1)
])

# Definition der Kosten-(Loss-)Funktion und der Optimierungsfunktion mit seinen Hyperparametern
model.compile(loss="mse", optimizer=tf.keras.optimizers.Adam(0.001))

# Ausgabe einer Zusammenfassung zur Form des Modells, das geschaetzt wird (nicht notwendig)
model.summary()
```

### Schätzen neuronales Netz
```{python}
# Schaetzung des Modells
history = model.fit(r.train_dataset, r.train_labels, epochs=150,
                    validation_data = (r.test_dataset, r.test_labels), verbose=0)

# Ggf. Speichern des geschaetzten Modells
model.save("python_model.h5")

```
## Auswertung der Modelloptimierung
```{r}
# Grafische Ausgabe der Modelloptimierung

# create data
data <- data.frame(val_loss = unlist(py$history$history$val_loss),
                  loss = unlist(py$history$history$loss))

# Plot
ggplot(data[-1,]) +
  geom_line( aes(x=1:length(val_loss), y=val_loss, colour = "Validation Loss" )) +
  geom_line( aes(x=1:length(loss), y=loss, colour = "Training Loss" )) +
  scale_colour_manual( values = c("Training Loss"="blue", "Validation Loss"="red") ) +
  labs(title="Loss Function Values During Optimization") +
  xlab("Iteration Number") +
  ylab("Loss") 

```

## Auswertung der Schätzergebnisse
```{r}
# Schätzung der (normierten) Preise für die Trainings- und Testdaten
train_predictions_norm <- py$model$predict(train_dataset)
test_predictions_norm <- py$model$predict(test_dataset)

# Rückberechnung der normierten Preisschätzungen zu den tatsächlichen Preisschätzungen bzw. Preisen
train_predictions <- (train_predictions_norm * norm_values_list$sd[1] ) + norm_values_list$mean[1]
test_predictions <- (test_predictions_norm * norm_values_list$sd[1]) + norm_values_list$mean[1]
# Selektion der zugehörigen tatsächlichen Preise
train_actuals <- umsatzdaten$Umsatz[train_ind]
test_actuals <- umsatzdaten$Umsatz[train_ind]

# Vergleich der Gütekriterien für die Traingings- und Testdaten
mape(train_actuals, train_predictions)
     
cat(paste0("MAPE on the Training Data:\t", format(mape(train_actuals, train_predictions)*100, digits=3, nsmall=2)))
cat(paste0("\nMAPE on the Validation Data:\t", format(mape(test_actuals, test_predictions)*100, digits=3, nsmall=2)))

```

```{r}
## Grafischer vergleich der vorhergesagten und der tatsächlichen Preise für die Trainings- und Testdaten

# Zusammenstellung der Daten für die Plots
data_train <- data.frame(prediction = train_predictions/1000, actual = train_actuals/1000)
data_test <- data.frame(prediction = test_predictions/1000, actual = test_actuals/1000)



# Plot der Ergebnisse der Trainingsdaten
ggplot(data_train[1:100,]) +
  geom_line( aes(x=1:length(prediction), y=prediction, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(actual), y=actual, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Training Data") +
  xlab("Case Number") +
  ylab("Price in 1.000 USD") 

# Plot der Ergebnisse der Testdaten
ggplot(data_test[1:100,]) +
  geom_line( aes(x=1:length(prediction), y=prediction, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(actual), y=actual, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Test Data") +
  xlab("Case Number") +
  ylab("Price in 1.000 USD") 

```

```{r}
# Vorhersage für einen einzelnen Fall
cat(paste0("Vorhergesagter Preis:\t", round(test_predictions[100])))
cat(paste0("\nTatsächlicher Preis:\t", test_actuals[100]))
```


# .............................................................................. ----
# Merkliste für nützliche Funktionen

```{r}

glance() # um Parameter von lm() in Tabelle zu oacken
mape () # Fehler berechnen

```